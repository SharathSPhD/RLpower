\begin{abstract}
We present \textbf{sCO2RL}, an end-to-end reinforcement-learning (RL) framework
for autonomous control of a simple-recuperated supercritical CO$_2$ (sCO$_2$)
Brayton cycle recovering waste heat from steel industry electric-arc
and basic-oxygen furnace exhaust (200--1{,}200\textdegree{}C).
The pipeline integrates physics-faithful FMI~2.0 Co-Simulation (FMU) via
OpenModelica, a Gymnasium environment with a 7-phase structured curriculum,
Proximal Policy Optimisation (PPO) with Lagrangian constraint multipliers for
safe operation near the CO$_2$ critical point,
a Fourier Neural Operator (FNO) surrogate for GPU-accelerated training,
and a sub-millisecond TensorRT-FP16 deployment path.

After resolving five non-trivial software engineering defects in the training
infrastructure — covering observation normalisation persistence, episode
boundary detection, reward unit double-scaling, stale disturbance profiles,
and constraint-violation gating — the agent successfully traverses all seven
curriculum phases within 229{,}376 steps and completes a 5{,}013{,}504-step
training run with final in-training evaluation reward of \textbf{412.7} and
zero constraint violations.
In the post-training per-phase evaluation, the final policy outperforms PID
by \textbf{+24--29\%} in steady-state and moderate-transient phases (Phases~0--2)
while underperforming in severe-transient phases (Phases~3--6), a result
we attribute to catastrophic forgetting caused by spending 95\% of training
in Phase~6 alone — an identified failure mode with a clear remediation path.
Critically, the Lagrangian constraint mechanism maintains \textbf{zero safety
violations across all 70~evaluation episodes} in all 7~phases.
The deployment path achieves a p99 host latency of \textbf{0.046~ms},
exceeding the 1~ms plant-edge SLA by a factor of $22\times$.

To our knowledge this is the first publicly available framework combining
OpenModelica-exported sCO$_2$ FMU simulation, structured curriculum RL,
Lagrangian safe constraints, and a TensorRT deployment path.
All code, configurations, and pre-trained artefacts are released at
\url{https://github.com/SharathSPhD/RLpower} (MIT licence).
\end{abstract}
