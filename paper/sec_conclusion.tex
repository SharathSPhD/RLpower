\section{Conclusion and Future Work}
\label{sec:conclusion}
%% ---------------------------------------------------------------------------

This paper presents sCO2RL, a complete end-to-end reinforcement learning pipeline
for autonomous control of a supercritical CO$_2$ recompression Brayton cycle
recovering waste heat from steel industry furnace exhaust.
The system integrates physics-faithful FMU simulation, structured curriculum RL
with Lagrangian safety constraints, NVIDIA PhysicsNeMo FNO surrogate training,
and TensorRT-FP16 deployment — the first such openly-published combination for
sCO$_2$ WHR applications.

\textbf{Key empirical findings.}
\begin{itemize}
  \item \textbf{RL surpasses Ziegler--Nichols PID on well-trained phases.}
        The 5{,}013{,}504-step policy achieves $+30.3\%$, $+30.4\%$, and
        $+39.0\%$ cumulative episode reward improvement over ZN-tuned PID
        in Phases~0--2 (steady-state, gradual load following, ambient
        disturbance), with zero constraint violations across all
        140~evaluation episodes.
        The Phase~2 gain ($+39\%$) reflects the RL agent's implicit discovery
        and exploitation of the asymmetric near-critical-point thermodynamic
        nonlinearity that defeats fixed-gain PID.

  \item \textbf{Curriculum imbalance limits Phases 3--6.}
        Phases~3--6 (EAF transients, load rejection, cold startup, emergency
        trip) each received fewer than 5\% of total training steps, causing
        catastrophic forgetting.
        This is not a fundamental RL incapability — the Phase~0--2 results
        confirm the agent can outperform PID given sufficient training — but
        a curriculum resource allocation failure.

  \item \textbf{Interleaved replay causes catastrophic interference.}
        A 30\% replay ratio applied directly to the Phase~6-specialised
        5M checkpoint produced universal performance regression across all
        phases, including Phase~6 itself.
        A gradual cosine-annealed schedule (starting at $\leq$5\%) is
        strongly recommended for future replay experiments.

  \item \textbf{Safety is unconditionally maintained.}
        Zero CO$_2$ critical-point constraint violations across all 210
        evaluation episodes (140 final policy + 70 interleaved policy)
        confirms that the Lagrangian safety mechanism functions correctly
        regardless of reward-level policy quality.

  \item \textbf{FNO data quality is decisive.}
        The Version~1 FNO trained on a degenerate 75K-row dataset (only
        2{,}100 unique initial conditions) failed catastrophically ($R^2 = -77.15$).
        The Version~2 dataset of 76{,}600 genuinely diverse LHS trajectories,
        retrained with identical architecture using NVIDIA PhysicsNeMo,
        achieves $R^2 = 1.000$ and normalised RMSE~$= 0.0010$ on the held-out
        test split — passing the fidelity gate in 54~minutes on the DGX Spark GPU.
        This 200$\times$ improvement in RMSE with identical architecture and
        hyperparameters demonstrates unequivocally that dataset diversity
        dominates model architecture in determining FNO surrogate quality.

  \item \textbf{Deployment is production-ready.}
        TensorRT FP16 achieves p99~$= 0.046$~ms, $22\times$ under the 1~ms
        SLA, at ${\approx}29{,}600$~queries/s — sufficient to serve multiple
        plant instances simultaneously.
\end{itemize}

\textbf{Practitioner guidance.}
The five engineering defects documented in Section~\ref{sec:bugs} are the
most directly applicable contribution for practitioners integrating Modelica
FMUs with RL training libraries.
Three of the five (normalisation persistence, episode boundary detection,
reward unit double-scaling) are not sCO$_2$-specific — they are latent
failure modes in any FMU-Gym-SB3 integration, and existing frameworks
(ModelicaGym, FMUGym, BOPTEST-Gym) do not address them.
Including detection strategies alongside the bugs themselves transforms this
from a chronicle of failures into an actionable debugging reference.

\textbf{Future work.}
\begin{enumerate}
  \item \textbf{Rebalanced curriculum allocation.}
        Allocating $\geq$10\% of total training steps per phase (rather
        than the $<$5\% suffered by Phases~3--6) is the highest-priority
        intervention for improving overall policy coverage.
        Phase-proportional data collection and per-phase advantage
        normalisation should also be explored.

  \item \textbf{Continual learning for multi-phase retention.}
        Elastic weight consolidation~\cite{kirkpatrick2017ewc} or
        progressive neural networks~\cite{rusu2016progressive} would allow
        sequential phase deepening without catastrophic forgetting, without
        requiring the careful replay scheduling needed by the interleaved
        approach.

  \item \textbf{FNO surrogate GPU training path.}
        The Version~2 FNO (76{,}600 unique LHS trajectories, NVIDIA
        PhysicsNeMo, $R^2 = 1.000$) is trained and validated, enabling
        GPU-vectorised PPO at ${\approx}10^6$~steps/s.
        Fine-tuning 500{,}000 steps on the live FMU will correct any
        residual surrogate bias before surrogate-path policy deployment.
        The ${\approx}1{,}250\times$ throughput increase would allow
        exhaustive hyperparameter searches — particularly for curriculum
        scheduling — that are currently infeasible on the CPU FMU path.

  \item \textbf{Recompression topology.}
        Extending to a full recompression Brayton cycle (adding a secondary
        compressor and flow-split control) would bring the simulation closer
        to utility-scale sCO$_2$ installations.

  \item \textbf{Multi-objective deployment.}
        Incorporating electricity price and grid frequency signals into
        the reward function would enable economically optimal dispatch
        — a necessary step for field deployment.
\end{enumerate}

%% ---------------------------------------------------------------------------
