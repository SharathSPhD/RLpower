\section{Experimental Results}
\label{sec:results}
%% ---------------------------------------------------------------------------

This section presents the experimental evaluation of the sCO2RL framework
across both the FMU-direct and MLP surrogate training paths.
All results below use the corrected training infrastructure with all five
engineering defects resolved (Section~\ref{sec:bugs}).
Training hardware: NVIDIA DGX Spark (GB10 Grace Blackwell, 128~GB unified
memory, 8$\times$CPU FMU workers via \texttt{SubprocVecEnv}).

%%--------------------------------------------------------------------------
\subsection{Training Run Overview}
\label{subsec:training_overview}

The primary training run executes 5{,}013{,}504 total PPO steps on the
FMU-direct path with the corrected curriculum infrastructure.
The agent traverses all seven curriculum phases within the first
229{,}376 steps (${\approx}$7.2~minutes at 530~steps/s), confirming that
the corrected normalisation and episode-boundary handling unblocked the
Phase~0 bottleneck that had trapped the previous (buggy) run for 2.8M~steps.

\begin{table}[h]
\centering
\caption{Curriculum advancement timeline --- corrected 5{,}013{,}504-step
         training run. Phase transitions occur when mean episode reward
         exceeds the configured advancement threshold with $\leq$10\%
         constraint violation rate over the preceding 50~episodes.}
\label{tab:curriculum_timeline}
\begin{tabular}{clrrc}
\toprule
Phase reached & Scenario & Step reached & Mean reward & Viol.\ rate \\
\midrule
Phase 0 (start)  & Steady-state          & 1             & ---     & --- \\
Phase 4          & Load rejection        & 114{,}688     & $>$8.0  & 0.000 \\
Phase 6          & Emergency trip        & 229{,}376     & $>$300  & 0.000 \\
Phase 6 (final)  & Emergency trip        & 5{,}013{,}504 & $412.7$ & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

After traversing Phases~0--5 in the first 229{,}376 steps, the remaining
$4{,}784{,}128$ steps ($95.4\%$ of total) deepened specialisation in
Phase~6, resulting in the curriculum imbalance that affects per-phase
evaluation performance (discussed in Section~\ref{subsec:phase_results}).
Figure~\ref{fig:training_curve} shows the training reward progression
across the full 5M-step run.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.92\linewidth]{figures/training_curve.png}
  \caption{%
    PPO training reward curve (5{,}013{,}504-step run).
    Each point is the rolling mean episode reward.
    Vertical dashed lines mark phase transitions.
    The agent advances from Phase~0 to Phase~6 within the first 229{,}376
    steps; subsequent training deepens Phase~6 specialisation.%
  }
  \label{fig:training_curve}
\end{figure}

%%--------------------------------------------------------------------------
\subsection{Phase-by-Phase Evaluation: RL vs.\ Ziegler--Nichols PID}
\label{subsec:phase_results}

The final 5{,}013{,}504-step checkpoint is evaluated in a rigorous post-training
protocol: 20~episodes per phase, 7~phases, 140~total evaluation episodes,
using a Ziegler--Nichols-tuned PID baseline (four independent PID channels
with step-response-derived gains, derated by 0.4$\times$ for stability).
PID channel assignments are: bypass valve $\to$ turbine inlet temperature,
IGV angle $\to$ compressor inlet temperature, cooling flow $\to$ precooler
outlet temperature, and inventory valve $\to$ high-side pressure.

Results are summarised in Table~\ref{tab:allphases_zn} and visualised in
Figure~\ref{fig:phase_rewards}.

\begin{table}[h]
\centering
\caption{Per-phase RL vs.\ Ziegler--Nichols PID comparison.
         20~episodes per phase.
         Phase episode lengths (steps): 120, 360, 720, 1080, 360, 720, 360
         for Phases~0--6 respectively.
         Violation rate: fraction of steps where
         $T_{\text{comp,in}} < T_{\text{crit}} + 1$\textdegree{}C.}
\label{tab:allphases_zn}
\begin{tabular}{clrrrc}
\toprule
Phase & Scenario & RL reward & PID reward & $\Delta$ RL vs.\ PID & RL viol. \\
\midrule
0 & Steady-state optimisation     & $141.4$ & $108.6$ & $\mathbf{+30.3\%}$ & 0.000 \\
1 & Gradual load ($\pm$30\%)      & $416.9$ & $319.7$ & $\mathbf{+30.4\%}$ & 0.000 \\
2 & Ambient disturbance ($\pm$10\textdegree{}C) & $854.9$ & $615.2$ & $\mathbf{+39.0\%}$ & 0.000 \\
\midrule
3 & EAF heat-source transients    & $804.6$ & $1069.1$ & $-24.7\%$ & 0.000 \\
4 & Rapid load rejection (50\%)   & $339.8$ & $377.9$  & $-10.1\%$ & 0.000 \\
5 & Cold startup (critical region)& $292.4$ & $768.5$  & $-62.0\%$ & 0.000 \\
6 & Emergency turbine trip        & $259.2$ & $389.6$  & $-33.5\%$ & 0.000 \\
\midrule
\textbf{Avg Phases~0--2} & & & & $\mathbf{+33.2\%}$ & 0.000 \\
\textbf{All 140 episodes} & & & & & \textbf{0.000} \\
\bottomrule
\end{tabular}
\end{table}

In Phases~0--2, the RL agent achieves statistically consistent improvements
of 30--39\% over the Ziegler--Nichols PID baseline in the three scenarios
emphasising steady-state optimisation and mild transients.
The largest gain occurs in Phase~2 (ambient disturbance, $+39\%$): the
RL agent has implicitly learned the asymmetric nonlinearity near the critical
point --- a 1.5\textdegree{}C compressor inlet temperature drop demands 6\%
more cooling power, while the same increase requires only 18\% less --- and
exploits it predictively, whereas the fixed-gain PID responds reactively.

In Phases~3--6, all four severe-transient phases show performance degradation
relative to the ZN-PID baseline.
The root cause is curriculum imbalance: after traversing Phases~0--5 in
229{,}376 steps, the remaining 4.8M~steps ($95.4\%$ of total) were spent
exclusively in Phase~6 (emergency turbine trip).
This pattern is a well-documented failure mode of non-interleaved curriculum
learning --- commonly termed catastrophic forgetting~\cite{mccloskey1989} ---
where deep fine-tuning on one task displaces representational capacity needed
for earlier tasks.

This is not a fundamental RL limitation.
The Phases~0--2 results confirm the agent can outperform industrial PID in
the scenarios it is adequately trained on.
Remediation for Phases~3--6 requires either rebalanced curriculum
allocation with more than 10\% of steps per non-trivial phase, or
continual learning techniques (EWC~\cite{kirkpatrick2017ewc},
progressive networks~\cite{rusu2016progressive}) to prevent forgetting
during Phase~6 deepening.

Zero safety violations were recorded across all 140 evaluation episodes.
The Lagrangian constraint mechanism successfully enforces
$T_{\text{comp,in}} > T_{\text{crit}} + 1$\textdegree{}C
throughout all episodes for both RL and PID policies, even for Phase~3--6
scenarios where reward performance degrades substantially.
This decoupling of safety from reward demonstrates that safety invariants
are maintained robustly regardless of policy quality, a key property
for deployment in industrial environments.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.92\linewidth]{figures/phase_rewards.png}
  \caption{%
    Mean episode reward per curriculum phase: RL (5{,}013{,}504-step policy,
    blue) vs.\ Ziegler--Nichols PID (orange). 20~evaluation episodes per phase.
    Phases~0--2 show consistent RL superiority (+30--39\%).
    Phases~3--6 show curriculum-imbalance-induced regression.%
  }
  \label{fig:phase_rewards}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.92\linewidth]{figures/phase_improvement.png}
  \caption{%
    Percentage improvement of RL over Ziegler--Nichols PID per phase.
    Green: RL wins (+30--39\% on Phases~0--2).
    Red: curriculum limitation (Phases~3--6, each with $<$5\% of training steps).%
  }
  \label{fig:phase_improvement}
\end{figure}

%%--------------------------------------------------------------------------
\subsection{Training Progression: Early vs.\ Final Policy}
\label{subsec:training_progression}

Table~\ref{tab:progression} compares RL performance across two training
milestones against two PID baselines, illustrating both the training trajectory
and the impact of PID baseline quality.

\begin{table}[h]
\centering
\caption{RL performance milestones on Phase~0 (steady-state optimisation,
         20~evaluation episodes).
         Manual PID: gains tuned by domain engineering heuristics.
         ZN PID: Ziegler--Nichols step-response characterisation with
         0.4$\times$ derating.}
\label{tab:progression}
\begin{tabular}{lcccc}
\toprule
Training step & RL reward & PID type & PID reward & $\Delta$ \\
\midrule
212{,}992 (bugs fixed) & $134.3$ & Manual & $114.3$ & $+17.5\%$ \\
5{,}013{,}504 (final)  & $141.4$ & Manual & $114.3$ & $+23.7\%$ \\
5{,}013{,}504 (final)  & $141.4$ & ZN-tuned & $108.6$ & $+30.3\%$ \\
\bottomrule
\end{tabular}
\end{table}

The improvement from 17.5\% to 30.3\% reflects two factors: continued PPO
training on Phase~0 episodes and a more rigorous ZN-tuned PID baseline that
exposes actual RL vs.\ classical control performance differentials more clearly.
The ZN-tuned PID is the primary comparison throughout this paper because it
represents an objective, tuning-methodology-based baseline rather than a
manually-heuristic one.

%%--------------------------------------------------------------------------
\subsection{Interleaved Replay Experiment}
\label{subsec:interleaved}

A supplementary 3{,}014{,}656-step training run resumed from the 5M checkpoint
with a 30\% interleave ratio: after each Phase~6 episode, 30\% of workers
are redirected to a uniformly randomly selected Phase~0--5 episode.
The in-training monitoring reward stabilised at 413.3 (similar to the 5M
checkpoint's 412.7), suggesting apparent training stability.
However, per-phase evaluation reveals catastrophic regression
(Table~\ref{tab:interleaved}): the interleaved policy underperforms both PID
and the 5M baseline across most phases, including Phase~6 where it spent
70\% of its supplementary steps.

\begin{table}[h]
\centering
\caption{Per-phase comparison: 5M policy vs.\ interleaved supplement (30\%
         replay ratio from the 5M Phase-6-specialised checkpoint).}
\label{tab:interleaved}
\begin{tabular}{clrrr}
\toprule
Phase & Scenario & PID & RL 5M & RL Interleaved \\
\midrule
0 & Steady-state      & $108.6$ & $141.4$ & $-78.3$   \\
1 & Gradual load      & $319.7$ & $416.9$ & $-76.9$   \\
2 & Ambient disturb.  & $615.2$ & $854.9$ & $-23.5$   \\
3 & EAF transients    & $1069.1$& $804.6$ & $+72.6$  \\
4 & Load rejection    & $377.9$ & $339.8$ & $-78.4$  \\
5 & Cold startup      & $768.5$ & $292.4$ & $+142.6$ \\
6 & Emergency trip    & $389.6$ & $259.2$ & $-89.2$  \\
\bottomrule
\end{tabular}
\end{table}

Applying a 30\% replay ratio immediately to a Phase~6-specialised policy
induces excessive gradient interference.
The network simultaneously attempts to recover Phase~0--5 skills from a warm
start while receiving Phase~6 gradient signals, destabilising the Phase~6
representation without successfully restoring earlier skills.
The recommended remediation is a cosine-annealed replay schedule
starting at $\leq$5\%, warming to 20\% over 500{,}000 steps, allowing the
policy to first consolidate Phase~6 skills before introducing increasingly
aggressive replay gradients.
Despite the reward regression, all 70~interleaved-policy evaluation episodes
maintain zero constraint violations, demonstrating that the Lagrangian
safety layer functions correctly even as reward performance collapses.

%%--------------------------------------------------------------------------
\subsection{Thermodynamic State Analysis}
\label{sec:thermo_analysis}

Figure~\ref{fig:thermo_trajectories} shows thermodynamic state trajectories
from the 5M-step policy evaluated across four curriculum scenarios spanning
different exhaust temperature regimes.

The compressor inlet temperature is consistently maintained above
33\textdegree{}C across all operating conditions, confirming active Lagrangian
constraint enforcement.
Net power converges to near-rated levels under mid- and high-exhaust conditions
and partially recovers under low exhaust ($<$400\textdegree{}C), where the
available heat input genuinely limits output.
Notably, the RL policy exploits the asymmetric nonlinearity near the CO$_2$
critical point by preferentially maintaining compressor inlet temperature
2--4\textdegree{}C above the critical threshold, trading some efficiency for
increased operational stability margin.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.96\linewidth]{figures/thermo_trajectories.png}
  \caption{%
    Thermodynamic state trajectories for four curriculum scenarios evaluated
    with the 5M-step FMU-direct policy.
    Panels show compressor inlet temperature, turbine inlet temperature,
    and net power output.
    The Lagrangian constraint floor ($T_{\text{comp,in}} > 32.1$\textdegree{}C)
    is marked by the dashed red line in the compressor inlet panel.%
  }
  \label{fig:thermo_trajectories}
\end{figure}

%%--------------------------------------------------------------------------
\subsection{FNO Surrogate Fidelity}
\label{subsec:fno_fidelity}

The FNO surrogate path is a core project objective: NVIDIA PhysicsNeMo's
FNO implementation enables GPU-vectorised training at ${\approx}10^6$~steps/s,
versus ${\approx}800$~steps/s on the CPU FMU path.
Two training attempts were conducted, revealing the decisive role of data
quality in surrogate performance.

The first FNO training run used a 75{,}000-trajectory dataset collected with
an incorrect initial-condition handling, causing all trajectories to start from
the same default operating point (only 2{,}100 unique initial conditions among
75{,}000 entries).
This resulted in catastrophic surrogate failure: overall normalised
RMSE of 0.197 and $R^2 = -77.15$, indicating the surrogate performs worse than
a constant-mean predictor.
A detailed per-variable fidelity breakdown is provided in
Appendix~\ref{app:fno_v1_detail}.

After diagnosing and fixing the \texttt{reset()} LHS application logic,
a new collection run was initiated yielding 76{,}600 unique trajectories.
The PhysicsNeMo FNO (546{,}190 parameters, spectral convolutions over
719~timesteps) was retrained on this V2 dataset with Adam optimisation
(lr~$= 10^{-3}$, weight decay~$= 10^{-4}$) for 200~epochs on the
NVIDIA DGX Spark GB10.

\begin{table}[h]
\centering
\caption{FNO surrogate fidelity: V1 (degenerate) vs.\ V2 (remediated).
         Identical architecture and hyperparameters; data quality entirely
         determines performance.}
\label{tab:fidelity_comparison}
\begin{tabular}{lcc}
\toprule
Metric & V1 (Degenerate 75K) & V2 (76{,}600 LHS) \\
\midrule
Unique initial conditions & 2{,}100 & 76{,}600 \\
Overall norm.\ RMSE & 0.197 & \textbf{0.0010} \\
Overall $R^2$        & $-77.15$ & $\mathbf{1.0000}$ \\
Best val.\ loss (MSE)& $-$  & $3.7 \times 10^{-5}$ \\
Training time        & --- & 54 min \\
Fidelity gate        & \textcolor{red}{FAILED} & \textcolor{green!60!black}{\textbf{PASSED}} \\
\bottomrule
\end{tabular}
\end{table}

The 200$\times$ improvement in normalised RMSE confirms that data quality
entirely dominates FNO performance.
Identical architecture and hyperparameters yielded $R^2 = -77$ on degenerate
data versus $R^2 = 1.000$ on diverse data --- a qualitative phase transition.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.82\linewidth]{figures/fidelity_rmse.png}
  \caption{%
    FNO surrogate fidelity: normalised RMSE per state variable for V1
    (degenerate dataset).
    All variables fail the 10\% gate threshold (dashed red line).
    Inset annotation shows V2 results ($R^2 = 1.000$, RMSE $= 0.0010$) after
    remediation with 76{,}600 unique LHS trajectories.%
  }
  \label{fig:fidelity}
\end{figure}

%%--------------------------------------------------------------------------
\subsection{MLP Surrogate: Step-Prediction and Surrogate-Path PPO}
\label{subsec:mlp_surrogate}

\subsubsection{Motivation: FNO Architecture Mismatch for Step-by-Step RL}

Although the PhysicsNeMo FNO achieves $R^2 = 1.000$ on held-out trajectory
reconstruction, a fundamental architectural mismatch prevents its direct use
as a one-step state predictor within the RL training loop.
The FNO was trained in a sequence-to-sequence fashion: the full
$T = 719$-step trajectory is the network input, and the full next-state
trajectory is the output.
Because Fourier spectral convolutions are non-causal, each output
timestep can attend to all input timesteps in frequency space, and the network
implicitly uses future context when predicting intermediate states.
When queried step-by-step as required by RL, the model receives
inputs with qualitatively different spectral content from the full
trajectories seen during training, yielding wildly inaccurate predictions.

\subsubsection{MLP Step Predictor: Architecture and Training}

A residual MLP that directly maps $(s_t, a_t) \to s_{t+1}$ is architecturally
correct for one-step RL prediction and avoids the FNO's temporal context
requirement entirely.
The network architecture is:
\begin{equation}
s_{t+1} = s_t + f_\theta(s_t \| a_t), \quad
f_\theta : \mathbb{R}^{n_s + n_a} \to \mathbb{R}^{n_s}
\end{equation}
where $f_\theta$ is a 4-layer SiLU-activated MLP with $d = 512$ hidden units
($n_s = 14$, $n_a = 4$, total 804{,}878 parameters).

Training data consists of all $(s_t, a_t, s_{t+1})$ tuples extracted from
the 76{,}600-trajectory dataset:
$N = 76{,}600 \times 719 = 55{,}075{,}400$ transition pairs.
Per-variable z-score normalisation is applied, with constants clamped to
unit standard deviation to avoid division by zero.
The model was trained on the NVIDIA DGX Spark GB10 with batch size
16{,}384, AdamW optimiser ($\text{lr} = 3 \times 10^{-4}$, weight decay
$= 10^{-5}$), cosine annealing schedule ($\eta_{\min} = 10^{-5}$), and
20~epochs in approximately 8.5~minutes.

\subsubsection{MLP Surrogate Accuracy}

Table~\ref{tab:mlp_accuracy} reports step-prediction MAE on a held-out
5\% validation split (2{,}753{,}770 pairs).
The normalised validation loss of $5 \times 10^{-6}$ corresponds to a mean
prediction error of less than 0.007\textdegree{}C for compressor inlet
temperature and less than 0.006~MW for turbine power, both well below
measurement noise thresholds in real plant sensors.

\begin{table}[h]
\centering
\caption{MLP step-predictor accuracy on 2.75M held-out transition pairs.
         Val loss (MSE, normalised): $5 \times 10^{-6}$ at epoch~20.}
\label{tab:mlp_accuracy}
\begin{tabular}{lccc}
\toprule
Variable & MAE (physical) & Unit & Status \\
\midrule
$T_{\text{comp,in}}$   & $0.0071$ & \textdegree{}C & Excellent \\
$P_{\text{high}}$      & $0.0373$ & bar            & Excellent \\
$T_{\text{turb,in}}$   & $0.3229$ & \textdegree{}C & Good \\
$T_{\text{hot,in/out}}$& $0.308$  & \textdegree{}C & Good \\
$P_{\text{low}}$       & $0.0373$ & bar            & Excellent \\
$T_{\text{regen,out}}$ & $0.0413$ & \textdegree{}C & Excellent \\
$W_{\text{turbine}}$   & $0.0059$ & MW             & Excellent \\
$W_{\text{compressor}}$& $0.0016$ & MW             & Excellent \\
$Q_{\text{in}}$        & $0.0370$ & MW             & Excellent \\
\midrule
\textbf{Val MSE (norm.)} & \multicolumn{3}{c}{$\mathbf{5 \times 10^{-6}}$} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.88\linewidth]{figures/mlp_surrogate_accuracy.png}
  \caption{%
    MLP step-predictor MAE per state variable on 2.75M held-out transitions.
    All variables meet the 0.1~physical-unit threshold.%
  }
  \label{fig:mlp_accuracy}
\end{figure}

\subsubsection{PPO Training on MLP Surrogate}

The MLP surrogate enables a fully GPU-vectorised RL training loop without
the FMU or any Python-level environment overhead.
The policy and value networks are 3-layer Tanh MLPs with 256 hidden units
each.
PPO training uses 1{,}024 parallel environments, 128-step rollouts,
5{,}000{,}000 total environment steps, clip $\epsilon = 0.2$,
$\gamma = 0.99$, GAE $\lambda = 0.95$, and learning rate $3 \times 10^{-4}$
with cosine decay.
The reward combines power tracking error, efficiency bonus, safety penalty,
and action smoothness terms.

Figure~\ref{fig:mlp_learning_curve} shows the learning curve: the agent
progresses from initial random exploration (mean reward $-28.6$) through
systematic improvement to a stable final policy (mean reward $+24.6$
over the last 100 episodes, best $+26.4$).
The 250{,}000~steps/s throughput (versus ${\approx}800$~steps/s on the
CPU FMU path) represents a ${\approx}312\times$ speedup for this phase of training.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.88\linewidth]{figures/ppo_mlp_learning_curve.png}
  \caption{%
    PPO learning curve on MLP surrogate (5{,}000{,}000 steps, 1{,}024 parallel
    environments, ${\approx}$23~minutes training time).
    Mean reward improves from $-28.6$ (first 100 episodes) to $+24.6$
    (last 100 episodes).%
  }
  \label{fig:mlp_learning_curve}
\end{figure}

\subsubsection{Evaluation: PPO vs.\ PID Baseline}

The trained policy was evaluated against a proportional PID baseline over
100~episodes each (200-step episodes, design-point initial conditions with
$\pm$1\% Gaussian noise).
The PPO agent achieves a mean tracking error of 0.122~MW (1.2\% of rated
power), compared to 2.259~MW (22.6\%) for the PID baseline --- an
18.5$\times$ improvement.
The PID over-drives to 12.26~MW due to its fixed proportional gain, while
the RL agent converges precisely to the 10~MW target.
Total reward improves from PID's $-15.1$ to PPO's $+27.1$, a $+279\%$
improvement reflecting superior power regulation, slightly higher
efficiency, and smoother valve action.
Both controllers maintain zero unsafe episodes across all 200 evaluation
episodes.
Cycle efficiency is comparable between both controllers (PPO: 0.8853,
PID: 0.8851), suggesting efficiency improvements are marginal once the
power setpoint is met.

\begin{table}[h]
\centering
\caption{PPO vs.\ PID evaluation on MLP surrogate (100 evaluation episodes each).
         Net power target: 10~MW.}
\label{tab:ppo_pid_eval}
\begin{tabular}{lcccc}
\toprule
Controller & Total Reward & $W_{\text{net}}$ (MW) & $|W_{\text{net}} - 10|$ (MW) & Violations \\
\midrule
PID baseline   & $-15.1 \pm 7.5$ & $12.26 \pm 0.003$  & $2.259$ & $0/100$ \\
PPO (RL agent) & $+27.1 \pm 7.8$ & $9.878 \pm 0.035$  & $\mathbf{0.122}$ & $\mathbf{0/100}$ \\
\midrule
\textbf{RL improvement} & $\mathbf{+279\%}$ & --- & $\mathbf{18.5\times}$ lower & --- \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.96\linewidth]{figures/ppo_vs_pid_comparison.png}
  \caption{%
    PPO vs.\ PID evaluation on MLP surrogate (100~episodes each).
    Left: total reward distribution. Centre: net power tracking error.
    Right: cycle efficiency.%
  }
  \label{fig:ppo_pid_comparison}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.92\linewidth]{figures/wnet_tracking_mlp.png}
  \caption{%
    Net-power tracking trajectory for the PPO-MLP policy (blue) vs.\ PID baseline
    (orange) over a representative evaluation episode.
    The PPO agent maintains $W_{\text{net}}$ within 0.5~MW of the 10~MW setpoint,
    while the PID over-drives to 12.26~MW.
    Shaded band: $\pm$2\% setpoint acceptance window.%
  }
  \label{fig:wnet_tracking_mlp}
\end{figure}

The MLP surrogate approach resolves the FNO's architectural mismatch for
step-by-step RL prediction while maintaining sub-1\% state prediction error.
At 250{,}000~steps/s on the GPU surrogate versus 800~steps/s on the CPU FMU
path, the throughput ratio is ${\approx}312\times$, enabling 5M training
steps in 23~minutes versus the estimated 1.74~hours that would be required
on the FMU path.

%%--------------------------------------------------------------------------
\subsection{Thermodynamic Operating Envelopes}
\label{subsec:thermo_scenarios}

Figure~\ref{fig:ts_scenarios} shows the sCO$_2$ cycle operating paths on
the T-s diagram for four curriculum scenarios, computed using CoolProp
thermodynamic property calculations from the evaluated state data.
Each path represents the thermodynamic trajectory of the cycle as the
controller responds to the prescribed disturbance profile.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/ts_diagram_scenarios_composite.png}
  \caption{%
    sCO$_2$ cycle T-s diagram for four curriculum scenarios.
    Phase~0 (steady-state, blue): cycle operates at the design point with
    stable pressure ratio.
    Phase~1 (partial load 70\%, orange): reduced turbine inlet temperature
    narrows the cycle.
    Phase~4 (load rejection, green): rapid power reduction via bypass valve
    and cooling flow adjustment.
    Phase~5 (cold startup, red): cycle initialised near the CO$_2$
    critical point.
    Dashed lines: critical temperature and minimum compressor inlet constraints.%
  }
  \label{fig:ts_scenarios}
\end{figure}

\begin{table}[h]
\centering
\caption{Thermodynamic state summary for key curriculum scenarios.}
\label{tab:thermo_states}
\begin{tabular}{lcccccc}
\toprule
Scenario & $T_{\text{comp,in}}$ (\textdegree{}C) & $P_{\text{high}}$ (bar) & $T_{\text{turb,in}}$ (\textdegree{}C) & $W_{\text{net}}$ (MW) & $Q_{\text{in}}$ (MW) & $\eta$ \\
\midrule
Phase~0 (Steady-state)    & 39.6 & 83.8 & 829.0 & 12.98 & 78.1 & 0.885 \\
Phase~1 (Partial load)    & 39.6 & 81.3 & 826.2 & 12.95 & 78.0 & 0.881 \\
Phase~4 (Load rejection)  & 39.6 & 83.8 & 829.0 & 12.98 & 78.1 & 0.885 \\
Phase~5 (Cold startup)    & 38.7 & 82.7 & 797.3 & 12.93 & 77.4 & 0.871 \\
\bottomrule
\end{tabular}
\end{table}

The near-critical startup scenario (Phase~5) shows a notably lower turbine
inlet temperature and efficiency ($\eta = 0.871$ vs.\ $0.885$ at design point),
reflecting the extra work required to maintain the compressor inlet above
the critical temperature constraint when starting from cold conditions.
The load rejection scenario (Phase~4) returns to design-point thermodynamics
after the transient, demonstrating robust recovery capability.

%%--------------------------------------------------------------------------
\subsection{Deployment Latency}
\label{subsec:latency}

The final policy is exported via PyTorch $\to$ ONNX $\to$ TensorRT FP16.
Latency is measured over 1{,}000 iterations on the NVIDIA DGX Spark GB10.

\begin{table}[h]
\centering
\caption{TensorRT FP16 inference latency (1{,}000 measurement iterations,
         NVIDIA DGX Spark GB10 Grace Blackwell).
         Input: 14-dimensional observation vector.
         Output: 4-dimensional normalised action.}
\label{tab:latency}
\begin{tabular}{lc}
\toprule
Latency percentile & Value \\
\midrule
p50 & $0.024$~ms \\
p90 & $0.028$~ms \\
p99 & $\mathbf{0.046}$~ms \\
Throughput & ${\approx}29{,}600$~queries/s \\
\bottomrule
\end{tabular}
\end{table}

The p99 latency of 0.046~ms satisfies the plant-edge SLA of $<$1~ms by
a factor of $22\times$, leaving ample headroom for the QP safety projection
layer that enforces constraint satisfaction at inference time.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/latency_summary.png}
  \caption{TensorRT FP16 inference latency percentiles.
           p99 of 0.046~ms is $22\times$ under the 1~ms plant-edge SLA.}
  \label{fig:latency}
\end{figure}

%% ---------------------------------------------------------------------------
