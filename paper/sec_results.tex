\section{Experimental Results}
\label{sec:results}
%% ---------------------------------------------------------------------------

All results below use the corrected training infrastructure with all five bugs
resolved (Section~\ref{sec:bugs}).
Hardware: NVIDIA DGX Spark (GB10 Grace Blackwell, 128~GB unified memory,
8$\times$CPU FMU workers via \texttt{SubprocVecEnv}).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.92\linewidth]{figures/training_curve.png}
  \caption{%
    PPO training reward curve.
    Each point is the rolling mean episode reward as logged by the
    monitor daemon (\texttt{fmu\_monitor.log}).
    Phase advances are annotated with vertical dashed lines.
    The final Phase~6 training achieves a mean in-training reward of~412.7.%
  }
  \label{fig:training_curve}
\end{figure}

\subsection{Curriculum Traversal Speed}

With the corrected training stack, the agent traverses all 7 curriculum phases
within 229{,}376 total steps (${\approx}7.2$ minutes at 530~steps/s):

\begin{table}[h]
\centering
\caption{Curriculum advancement timeline (corrected 5M-step training run).}
\label{tab:curriculum_timeline}
\begin{tabular}{clcc}
\toprule
Phase reached & Step & Mean reward & Violation rate \\
\midrule
Phase 4 & 114{,}688 & $>8.0$ & 0.000 \\
Phase 6 & 229{,}376 & $>300.0$ & 0.000 \\
Phase 6 (completed) & 5{,}013{,}504 & $412.7$ & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

By contrast, the previous training run (with bugs present) remained in Phase~0
for the entire 2.8M-step run.
The corrected run's rapid advancement confirms that the policy warm-started
from the Phase-0-converged checkpoint already possessed the latent capability
to handle advanced transients; the curriculum infrastructure bugs alone had
prevented it from demonstrating this.
After traversing Phases 0--5 in the first 229K steps, the remaining
${\approx}4.8$M steps deepened specialisation in Phase~6 (emergency turbine
trip recovery), producing a final in-training evaluation reward of 412.7.

\subsection{Phase-by-Phase Evaluation: Final 5M-Step Policy}

\begin{table}[h]
\centering
\caption{Per-phase RL vs.\ PID comparison, final 5{,}013{,}504-step checkpoint
         (10~episodes each).}
\label{tab:allphases}
\begin{tabular}{clrrrc}
\toprule
Phase & Scenario & RL reward & PID reward & Improvement & RL viol. \\
\midrule
0 & Steady-state     & $141.4$ & $114.3$ & $\mathbf{+23.7\%}$ & 0.000 \\
1 & Gradual load     & $416.2$ & $335.6$ & $\mathbf{+24.0\%}$ & 0.000 \\
2 & Ambient disturb. & $854.9$ & $664.2$ & $\mathbf{+28.7\%}$ & 0.000 \\
3 & EAF transients   & $773.3$ & $1161.7$ & $-33.4\%$ & 0.000 \\
4 & Load rejection   & $338.6$ & $402.5$ & $-15.9\%$ & 0.000 \\
5 & Cold startup     & $284.6$ & $820.9$ & $-65.3\%$ & 0.000 \\
6 & Emergency trip   & $255.4$ & $416.0$ & $-38.6\%$ & 0.000 \\
\midrule
\textbf{Avg 0--2} & & & & $\mathbf{+25.5\%}$ & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

The results exhibit a clear and interpretable pattern.
For Phases~0--2 (steady-state optimisation, gradual load following, and ambient
temperature disturbance), the RL agent \emph{consistently outperforms the PID
baseline by 24--29\%}.
For Phases~3--6 (severe multi-timescale transients, cold startup, and emergency
trips), the agent underperforms PID.

\textbf{Root cause: curriculum phase imbalance.}
After traversing Phases~0--5 in the first 229{,}376 steps, the remaining
$4{,}784{,}128$ steps ($95.4\%$ of total training) specialised exclusively
in Phase~6.
This is a known failure mode of non-interleaved curriculum learning:
the policy undergoes \emph{catastrophic forgetting}~\cite{mccloskey1989} of
earlier phase skills.
The Phase~6 scenario (emergency turbine trip with rapid inventory ejection)
requires qualitatively different valve dynamics than the EAF transient and
cold-startup scenarios of Phases~3--5, so deeper Phase~6 training actively
displaces the representational capacity needed for those phases.

\textbf{Zero constraint violations across all phases} (RL violation rate~0.000
for all 70~evaluation episodes) confirms that the Lagrangian safety mechanism
functions correctly even for scenarios the final policy has not practised:
the constraint projection QP at inference time ensures safety invariants
are maintained regardless of policy quality.

\textbf{Remediation attempt — interleaved replay (3M supplementary steps).}
A 3{,}014{,}656-step supplementary training run resumed from the 5M checkpoint
with a 30\% interleave ratio: after each Phase-6 episode completion, 30\% of
workers are redirected to a uniformly randomly selected Phase~0--5 episode for
their next episode.
The supplementary run completed with in-training mean reward 413.3 and zero
violations, but the per-phase evaluation reveals severe regression:

\begin{table}[h]
\centering
\caption{Per-phase comparison for all three policies (10 episodes each).
         The interleaved policy shows universal regression relative to 5M,
         attributable to an overly aggressive 30\% replay ratio applied
         to an already highly-specialised Phase-6 checkpoint.}
\label{tab:interleaved}
\begin{tabular}{clrrrr}
\toprule
Phase & Scenario & PID & RL 5M & RL Interleaved & $\Delta$ vs 5M \\
\midrule
0 & Steady-state     & $114.3$ & $141.4$ & $-78.3$  & $-219.7$ \\
1 & Gradual load     & $335.6$ & $416.2$ & $-76.9$  & $-493.1$ \\
2 & Ambient disturb. & $664.2$ & $854.9$ & $-23.5$  & $-878.4$ \\
3 & EAF transients   & $1161.7$& $773.3$ & $+72.6$  & $-700.7$ \\
4 & Load rejection   & $402.5$ & $338.6$ & $-78.4$  & $-417.0$ \\
5 & Cold startup     & $820.9$ & $284.6$ & $+142.6$ & $-142.0$ \\
6 & Emergency trip   & $416.0$ & $255.4$ & $-89.2$  & $-344.6$ \\
\bottomrule
\end{tabular}
\end{table}

The interleaved policy underperforms both PID and the 5M baseline across
all phases.
\textbf{Diagnosis}: applying a 30\% replay ratio immediately to a policy that
spent 95\% of its 5M steps in Phase~6 produced excessive plasticity:
the network re-learned Phase-6 skills from a warm start while simultaneously
receiving conflicting gradient signals from 30\% Phase~0--5 replay episodes,
causing instability rather than knowledge retention.
The in-training mean reward (413.3) is misleading because it measures only
Phase-6 episodes — the policy simultaneously forgot Phase~6 skills under the
replay interference.
\textbf{Recommended remediation}: use a decay schedule for the replay ratio
(start at $\leq$10\%, decay to 0\% over the first 500K steps) to allow
the policy to first consolidate Phase-6 skills before introducing replay
gradients.
Alternatively, elastic weight consolidation (EWC)~\cite{kirkpatrick2017ewc}
or progressive neural networks~\cite{rusu2016progressive} offer structural
solutions that do not require replay scheduling.

Despite the regression, the Lagrangian constraint mechanism maintains
\textbf{zero safety violations} across all 70 interleaved-policy evaluation
episodes, demonstrating that safety enforcement is robust even as reward
performance degrades.

\subsection{Surrogate Fidelity}

The FNO surrogate was evaluated against 1{,}000 held-out FMU roll-outs
using the FMU-grounded fidelity gate.

\begin{table}[h]
\centering
\caption{FNO surrogate fidelity metrics (gate threshold: RMSE$<0.05$, $R^2>0.8$).}
\label{tab:fidelity}
\begin{tabular}{lccc}
\toprule
Variable & Norm.\ RMSE & $R^2$ & Passed \\
\midrule
$T_{\text{compressor,inlet}}$ & $0.241$ & $-112.3$ & No \\
$T_{\text{turbine,inlet}}$ & $0.198$ & $-89.7$ & No \\
$P_{\text{high}}$ & $0.168$ & $-52.6$ & No \\
$W_{\text{net}}$ & $0.142$ & $-41.2$ & No \\
\midrule
\textbf{Overall} & $\mathbf{0.197}$ & $\mathbf{-77.15}$ & \textbf{No} \\
\bottomrule
\end{tabular}
\end{table}

The negative $R^2$ values indicate the surrogate performs worse than a
constant-mean predictor.
Root cause: the 75{,}000-trajectory dataset was generated by upsampling a
smaller base set; the surrogate likely overfit to repeated sequences.
The FMU-direct PPO path was therefore used for all reported RL results.
Remediation: collect a strictly-unique LHS-sampled dataset with
$\geq100{,}000$ distinct FMU roll-outs and retrain from scratch.

\subsection{Deployment Latency}

\begin{table}[h]
\centering
\caption{TensorRT FP16 inference latency (1{,}000 measurement iterations,
         Phase~3 policy, NVIDIA DGX Spark GB10).}
\label{tab:latency}
\begin{tabular}{lc}
\toprule
Latency percentile & Value \\
\midrule
p50 & $0.038$~ms \\
p90 & $0.043$~ms \\
p99 & $\mathbf{0.046}$~ms \\
Throughput & ${\approx}29{,}600$~QPS \\
\bottomrule
\end{tabular}
\end{table}

The p99 latency of $0.046$~ms satisfies the plant-edge SLA of $<1$~ms by
a factor of ${\approx}22\times$, leaving ample headroom for the QP safety
projection layer at deployment.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/latency_summary.png}
  \caption{TensorRT FP16 inference latency percentiles.
           p99 of 0.046~ms is $22\times$ under the 1~ms plant-edge SLA.}
  \label{fig:latency}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.88\linewidth]{figures/phase_rewards.png}
  \caption{%
    RL vs.\ PID mean episode reward per curriculum phase.
    Green bars show the 5M-step baseline policy.
    Hatched region (Phases~3--6) indicates catastrophic forgetting from
    Phase~6 specialisation.
    Interleaved-replay results will be added when the 3M supplementary
    run completes (run in progress).%
  }
  \label{fig:phase_rewards}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.88\linewidth]{figures/phase_improvement.png}
  \caption{%
    Percentage improvement of RL over PID per phase.
    Phases~0--2 show consistent gains (24--29\%).
    Phases~3--6 show regression attributable to catastrophic forgetting
    during Phase-6 specialisation.%
  }
  \label{fig:phase_improvement}
\end{figure}

\subsection{Thermodynamic State Analysis}
\label{sec:thermo_analysis}

Figure~\ref{fig:thermo_trajectories} shows thermodynamic state trajectories
from a preflight evaluation of 100 episodes covering three exhaust temperature
regimes: low ($<$400\textdegree{}C), mid (400--800\textdegree{}C), and high
($\geq$800\textdegree{}C), drawn from the 5M-step policy.

Key observations:
\begin{itemize}
  \item \textbf{Critical-point safety}: The compressor inlet temperature
        (top-left panel) is consistently maintained above 33\textdegree{}C
        across all heat source conditions, confirming that the Lagrangian
        constraint mechanism actively guards the CO$_2$ critical region
        throughout 100 distinct episodes.
  \item \textbf{Power output}: Net power (top-right) converges to near-rated
        10~MW under mid- and high-exhaust conditions and partially recovers
        under low exhaust ($<$400\textdegree{}C), where the available heat
        input genuinely limits output.
  \item \textbf{Thermal efficiency}: The thermal efficiency (lower-left)
        stabilises at 35--42\% at design exhaust conditions, consistent with
        the simple recuperated cycle's theoretical 40\% peak.
  \item \textbf{Recuperator effectiveness}: Hot outlet temperatures
        (lower-right) show steady convergence, indicating the recuperator
        operates near its design temperature crossover without thermal shock.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.96\linewidth]{figures/thermo_trajectories.png}
  \caption{%
    Thermodynamic state trajectories for 100 preflight evaluation episodes,
    grouped by initial EAF/BOF exhaust temperature.
    Mean $\pm$ 1$\sigma$ bands shown.
    The critical-constraint floor (compressor inlet $>$33\textdegree{}C)
    is marked by the dashed red line in the top-left panel.%
  }
  \label{fig:thermo_trajectories}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.82\linewidth]{figures/fidelity_rmse.png}
  \caption{%
    FNO surrogate fidelity: normalised RMSE per state variable
    vs.\ held-out FMU roll-outs.
    All variables exceed the 5\% gate threshold (dashed red),
    indicating the surrogate is not sufficiently accurate for
    surrogate-path PPO training; the FMU-direct path was used
    for all reported RL results.%
  }
  \label{fig:fidelity}
\end{figure}

%% ---------------------------------------------------------------------------
