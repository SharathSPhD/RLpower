\begin{abstract}
This paper presents \textbf{sCO2RL}, an end-to-end deep reinforcement learning
(RL) framework for autonomous control of a \emph{supercritical CO$_2$ (sCO$_2$)
recompression Brayton cycle} recovering waste heat from steel industry electric-arc
furnace (EAF) and basic-oxygen furnace (BOF) exhaust (200--1{,}200\textdegree{}C).
The framework integrates a physics-faithful FMI~2.0 Co-Simulation model (FMU)
compiled with OpenModelica, a Gymnasium environment with a seven-phase structured
curriculum, Proximal Policy Optimisation (PPO) with trainable Lagrangian constraint
multipliers for safe operation near the CO$_2$ critical point (31.1\textdegree{}C,
7.38~MPa), a Fourier Neural Operator (FNO) surrogate model implemented via
\textbf{NVIDIA PhysicsNeMo} for GPU-accelerated training, and a TensorRT-FP16
deployment path for sub-millisecond plant-edge inference.

The final policy is trained for 5{,}013{,}504 steps using the FMU-direct PPO
path on an NVIDIA DGX Spark (GB10 Grace Blackwell, 128~GB unified memory).
Post-training evaluation against a Ziegler--Nichols-tuned multi-channel PID
baseline (20~evaluation episodes per phase) reveals a clear and interpretable
performance pattern: the RL agent outperforms the PID baseline by
\textbf{+30.3\%, +30.4\%, and +39.0\%} in cumulative episode reward for
Phases~0--2 (steady-state optimisation, $\pm$30\% gradual load following, and
$\pm$10\textdegree{}C ambient temperature disturbance), while underperforming
in Phases~3--6 (EAF heat source transients, rapid load rejection, cold startup
through the critical region, and emergency turbine trip recovery).
This Phase~3--6 regression is attributed to \emph{curriculum imbalance}: these
phases collectively received fewer than 5\% of total training steps, causing
catastrophic forgetting of the associated control skills.
Critically, the Lagrangian constraint mechanism maintains
\textbf{zero safety violations across all 140~evaluation episodes} (20 episodes
$\times$ 7 phases, both RL and PID policies), confirming that safety invariants
are enforced robustly regardless of reward-level policy performance.

The FNO surrogate path revealed a critical data quality failure: a 75{,}000-%
trajectory dataset built by upsampling achieved only 2{,}100 unique initial
states, causing the FNO to overfit to repeated sequences (overall $R^2 = -77.15$).
Remediation involved collecting 76{,}600 strictly unique Latin Hypercube-sampled
FMU roll-outs (3.98~GB) and retraining using NVIDIA PhysicsNeMo's FNO
implementation (546{,}190 parameters, 200~epochs, 54~minutes on DGX Spark GPU).
The remediated FNO achieves an overall $R^2 = 1.000$ and normalised RMSE $= 0.0010$
on the held-out test split (7{,}660 trajectories), \emph{passing} the fidelity gate.
The deployment path achieves a p99 host latency of \textbf{0.046~ms},
exceeding the 1~ms plant-edge SLA by a factor of $22\times$.

Five non-trivial software engineering defects encountered during development
are documented in detail — covering observation normalisation persistence,
episode boundary detection, reward unit double-scaling, stale disturbance
profiles, and constraint-violation gating — as actionable practitioner guidance
for the Modelica-RL integration community.

To the authors' knowledge, this is the first publicly available framework
combining (i)~an OpenModelica-exported sCO$_2$ FMU, (ii)~structured curriculum
RL with Lagrangian safety constraints, (iii)~NVIDIA PhysicsNeMo FNO surrogate
integration, and (iv)~a TensorRT plant-edge deployment path.
All code, configurations, and pre-trained artefacts are released at
\url{https://github.com/SharathSPhD/RLpower} under the MIT licence.
\end{abstract}
