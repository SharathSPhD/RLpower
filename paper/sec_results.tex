\section{Experimental Results}
\label{sec:results}
%% ---------------------------------------------------------------------------

All results below use the corrected training infrastructure with all five
engineering defects resolved (Section~\ref{sec:bugs}).
Training hardware: NVIDIA DGX Spark (GB10 Grace Blackwell, 128~GB unified
memory, 8$\times$CPU FMU workers via \texttt{SubprocVecEnv}).

%%--------------------------------------------------------------------------
\subsection{Training Run Overview}
\label{subsec:training_overview}

The primary training run executes 5{,}013{,}504 total PPO steps on the
FMU-direct path with the corrected curriculum infrastructure.
The agent traverses all seven curriculum phases within the first
229{,}376 steps (${\approx}$7.2~minutes at 530~steps/s), confirming that
the corrected normalisation and episode-boundary handling unblocked the
Phase~0 bottleneck that had trapped the previous (buggy) run for 2.8M~steps.

\begin{table}[h]
\centering
\caption{Curriculum advancement timeline — corrected 5{,}013{,}504-step
         training run. Phase transitions occur when mean episode reward
         exceeds the configured advancement threshold with $\leq$10\%
         constraint violation rate over the preceding 50~episodes.}
\label{tab:curriculum_timeline}
\begin{tabular}{clrrc}
\toprule
Phase reached & Scenario & Step reached & Mean reward & Viol.\ rate \\
\midrule
Phase 0 (start)  & Steady-state          & 1             & ---     & --- \\
Phase 4          & Load rejection        & 114{,}688     & $>$8.0  & 0.000 \\
Phase 6          & Emergency trip        & 229{,}376     & $>$300  & 0.000 \\
Phase 6 (final)  & Emergency trip        & 5{,}013{,}504 & $412.7$ & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

After traversing Phases~0--5 in the first 229{,}376 steps, the remaining
$4{,}784{,}128$ steps ($95.4\%$ of total) deepened specialisation in
Phase~6, resulting in the curriculum imbalance that affects per-phase
evaluation performance (discussed in Section~\ref{subsec:phase_results}).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.92\linewidth]{figures/training_curve.png}
  \caption{%
    PPO training reward curve (5{,}013{,}504-step run).
    Each point is the rolling mean episode reward logged by the monitoring
    daemon.
    Vertical dashed lines mark phase transitions.
    The agent advances from Phase~0 to Phase~6 within the first 229{,}376
    steps; subsequent training deepens Phase~6 specialisation.%
  }
  \label{fig:training_curve}
\end{figure}

%%--------------------------------------------------------------------------
\subsection{Phase-by-Phase Evaluation: RL vs.\ Ziegler--Nichols PID}
\label{subsec:phase_results}

The final 5{,}013{,}504-step checkpoint is evaluated in a rigorous post-training
protocol: \textbf{20~episodes per phase}, \textbf{7~phases},
\textbf{140~total evaluation episodes}, using a
\textbf{Ziegler--Nichols-tuned PID baseline} (four independent PID channels
with step-response-derived gains, derated by 0.4$\times$ for stability).
PID channel assignments: bypass valve $\to$ turbine inlet temperature;
IGV angle $\to$ compressor inlet temperature; cooling flow $\to$ precooler
outlet temperature; inventory valve $\to$ high-side pressure.

Results are summarised in Table~\ref{tab:allphases_zn} and visualised in
Figure~\ref{fig:phase_rewards}.

\begin{table}[h]
\centering
\caption{Per-phase RL vs.\ Ziegler--Nichols PID comparison.
         20~episodes per phase.
         Phase episode lengths (steps): 120, 360, 720, 1080, 360, 720, 360
         for Phases~0--6 respectively.
         Violation rate: fraction of steps where
         $T_{\text{comp,in}} < T_{\text{crit}} + 1$\textdegree{}C.}
\label{tab:allphases_zn}
\begin{tabular}{clrrrc}
\toprule
Phase & Scenario & RL reward & PID reward & $\Delta$ RL vs.\ PID & RL viol. \\
\midrule
0 & Steady-state optimisation     & $141.4$ & $108.6$ & $\mathbf{+30.3\%}$ & 0.000 \\
1 & Gradual load ($\pm$30\%)      & $416.9$ & $319.7$ & $\mathbf{+30.4\%}$ & 0.000 \\
2 & Ambient disturbance ($\pm$10\textdegree{}C) & $854.9$ & $615.2$ & $\mathbf{+39.0\%}$ & 0.000 \\
\midrule
3 & EAF heat-source transients    & $804.6$ & $1069.1$ & $-24.7\%$ & 0.000 \\
4 & Rapid load rejection (50\%)   & $339.8$ & $377.9$  & $-10.1\%$ & 0.000 \\
5 & Cold startup (critical region)& $292.4$ & $768.5$  & $-62.0\%$ & 0.000 \\
6 & Emergency turbine trip        & $259.2$ & $389.6$  & $-33.5\%$ & 0.000 \\
\midrule
\textbf{Avg Phases~0--2} & & & & $\mathbf{+33.2\%}$ & 0.000 \\
\textbf{All 140 episodes} & & & & & \textbf{0.000} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Phases~0--2: RL clearly superior.}
The RL agent achieves statistically consistent improvements of 30--39\% over
the Ziegler--Nichols PID baseline in the three scenarios emphasising
steady-state optimisation and mild transients.
The largest gain occurs in Phase~2 (ambient disturbance, $+39\%$): the
RL agent has implicitly learned the asymmetric nonlinearity near the critical
point — a 1.5\textdegree{}C compressor inlet temperature drop demands 6\%
more cooling power, while the same increase requires only 18\% less — and
exploits it predictively, whereas the fixed-gain PID responds reactively.

\textbf{Phases~3--6: curriculum imbalance causes forgetting.}
All four severe-transient phases show performance degradation relative to the
ZN-PID baseline.
The root cause is curriculum imbalance: after traversing Phases~0--5 in
229{,}376 steps, the remaining 4.8M~steps ($95.4\%$ of total) were spent
exclusively in Phase~6 (emergency turbine trip).
This pattern is a well-documented failure mode of non-interleaved curriculum
learning — commonly termed catastrophic forgetting~\cite{mccloskey1989} —
where deep fine-tuning on one task displaces representational capacity needed
for earlier tasks.
The Phase~6 scenario (rapid pressure drop, turbine isolation, inventory ejection)
requires qualitatively different valve action dynamics than the EAF transient
and cold-startup scenarios, so Phase~6 specialisation actively degrades those
capabilities.

\textbf{This is not a fundamental RL limitation.}
The Phases~0--2 results confirm the agent can outperform industrial PID in
the scenarios it is adequately trained on.
Remediation for Phases~3--6 requires either (i)~rebalanced curriculum
allocation with $>$10\% of steps per non-trivial phase, or
(ii)~continual learning techniques (EWC~\cite{kirkpatrick2017ewc},
progressive networks~\cite{rusu2016progressive}) to prevent forgetting
during Phase~6 deepening.

\textbf{Zero safety violations across all 140 evaluation episodes.}
The Lagrangian constraint mechanism successfully enforces
$T_{\text{comp,in}} > T_{\text{crit}} + 1$\textdegree{}C
(CO$_2$ critical point plus 1\textdegree{}C margin) throughout all episodes
for both RL and PID policies, even for Phase~3--6 scenarios where reward
performance degrades substantially.
This decoupling of safety from reward demonstrates that safety invariants
are maintained robustly regardless of policy quality, a key property
for deployment in industrial environments.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.92\linewidth]{figures/phase_rewards.png}
  \caption{%
    Mean episode reward per curriculum phase: RL (5{,}013{,}504-step policy,
    blue) vs.\ Ziegler--Nichols PID (orange). 20~evaluation episodes per phase.
    Phases~0--2 show consistent RL superiority (+30--39\%).
    Phases~3--6 show curriculum-imbalance-induced regression (RL spent
    $<$5\% of training steps on each of these phases).%
  }
  \label{fig:phase_rewards}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.92\linewidth]{figures/phase_improvement.png}
  \caption{%
    Percentage improvement of RL over Ziegler--Nichols PID per phase.
    Green: RL wins (+30--39\% on Phases~0--2).
    Red: Curriculum limitation (Phases~3--6, each with $<$5\% of training steps).
    Dashed line shows early policy (212{,}992-step post-bug-fix checkpoint,
    Phase~0 only, $+17.5\%$ with manual PID baseline).%
  }
  \label{fig:phase_improvement}
\end{figure}

%%--------------------------------------------------------------------------
\subsection{Training Progression: Early vs.\ Final Policy}
\label{subsec:training_progression}

Table~\ref{tab:progression} compares RL performance across two training
milestones against two PID baselines, illustrating both the training trajectory
and the impact of PID baseline quality.

\begin{table}[h]
\centering
\caption{RL performance milestones on Phase~0 (steady-state optimisation,
         20~evaluation episodes).
         Manual PID: gains tuned by domain engineering heuristics.
         ZN PID: Ziegler--Nichols step-response characterisation with
         0.4$\times$ derating.}
\label{tab:progression}
\begin{tabular}{lcccc}
\toprule
Training step & RL reward & PID type & PID reward & $\Delta$ \\
\midrule
212{,}992 (bugs fixed) & $134.3$ & Manual & $114.3$ & $+17.5\%$ \\
5{,}013{,}504 (final)  & $141.4$ & Manual & $114.3$ & $+23.7\%$ \\
5{,}013{,}504 (final)  & $141.4$ & ZN-tuned & $108.6$ & $+30.3\%$ \\
\bottomrule
\end{tabular}
\end{table}

The improvement from 17.5\% to 30.3\% reflects two factors: continued PPO
training on Phase~0 episodes and a more rigorous ZN-tuned PID baseline that
exposes actual RL vs.\ classical control performance differentials more clearly.
The ZN-tuned PID is the primary comparison throughout this paper because it
represents an objective, tuning-methodology-based baseline rather than a
manually-heuristic one.

%%--------------------------------------------------------------------------
\subsection{Interleaved Replay Experiment}
\label{subsec:interleaved}

A supplementary 3{,}014{,}656-step training run resumed from the 5M checkpoint
with a 30\% interleave ratio: after each Phase~6 episode, 30\% of workers
are redirected to a uniformly randomly selected Phase~0--5 episode.
The in-training monitoring reward stabilised at 413.3 (similar to the 5M
checkpoint's 412.7), suggesting apparent training stability.
However, per-phase evaluation reveals catastrophic regression:

\begin{table}[h]
\centering
\caption{Per-phase comparison: 5M policy vs.\ interleaved supplement (30\%
         replay ratio from the 5M Phase-6-specialised checkpoint).
         The interleaved policy shows universal regression, attributable to
         excessive plasticity when conflicting gradient signals from 30\%
         Phase~0--5 replay immediately overwrite the highly-specialised
         Phase~6 policy.}
\label{tab:interleaved}
\begin{tabular}{clrrr}
\toprule
Phase & Scenario & PID & RL 5M & RL Interleaved \\
\midrule
0 & Steady-state      & $108.6$ & $141.4$ & $-78.3$   \\
1 & Gradual load      & $319.7$ & $416.9$ & $-76.9$   \\
2 & Ambient disturb.  & $615.2$ & $854.9$ & $-23.5$   \\
3 & EAF transients    & $1069.1$& $804.6$ & $+72.6$  \\
4 & Load rejection    & $377.9$ & $339.8$ & $-78.4$  \\
5 & Cold startup      & $768.5$ & $292.4$ & $+142.6$ \\
6 & Emergency trip    & $389.6$ & $259.2$ & $-89.2$  \\
\bottomrule
\end{tabular}
\end{table}

The interleaved policy underperforms both PID and the 5M baseline across
most phases, including Phase~6 where it spent 70\% of its supplementary
steps.
\textbf{Diagnosis}: applying a 30\% replay ratio immediately to a Phase~6-%
specialised policy induces excessive gradient interference.
The network simultaneously attempts to recover Phase~0--5 skills from a warm
start while receiving Phase~6 gradient signals, destabilising the Phase~6
representation without successfully restoring earlier skills.
The in-training metric (413.3) is misleading because it exclusively measures
Phase~6 performance — the reward signal does not observe the simultaneous
regression across other phases.

\textbf{Recommended remediation}: adopt a cosine-annealed replay schedule
starting at $\leq$5\%, warming to 20\% over 500{,}000 steps, allowing the
policy to first consolidate Phase~6 skills before introducing increasingly
aggressive replay gradients.
Alternatively, elastic weight consolidation (EWC)~\cite{kirkpatrick2017ewc}
or progressive neural networks~\cite{rusu2016progressive} offer structural
solutions that do not require replay scheduling.

Despite the reward regression, all 70~interleaved-policy evaluation episodes
maintain \textbf{zero constraint violations}, demonstrating that the Lagrangian
safety layer functions correctly even as reward performance collapses.

%%--------------------------------------------------------------------------
\subsection{Thermodynamic State Analysis}
\label{sec:thermo_analysis}

Figure~\ref{fig:thermo_trajectories} shows thermodynamic state trajectories
from the 5M-step policy evaluated across 100~episodes spanning three
exhaust temperature regimes: low ($<$400\textdegree{}C), mid
(400--800\textdegree{}C), and high ($\geq$800\textdegree{}C).

Key observations:
\begin{itemize}
  \item \textbf{Critical-point safety}: The compressor inlet temperature
        is consistently maintained above 33\textdegree{}C across all heat
        source conditions, confirming active Lagrangian constraint enforcement
        throughout 100~distinct episodes.

  \item \textbf{Power output}: Net power converges to near-rated 10~MW under
        mid- and high-exhaust conditions and partially recovers under low
        exhaust ($<$400\textdegree{}C), where the available heat input
        genuinely limits output.

  \item \textbf{Thermal efficiency}: The thermal efficiency stabilises at
        35--42\% at design exhaust conditions, consistent with the simple
        recuperated cycle's theoretical 40\% peak efficiency.

  \item \textbf{Recuperator effectiveness}: Hot outlet temperatures show
        steady convergence, indicating the recuperator operates near its
        design temperature crossover without thermal shock.

  \item \textbf{Asymmetric near-critical response}: The RL policy exploits
        the asymmetric nonlinearity near the CO$_2$ critical point by
        preferentially maintaining compressor inlet temperature 2--4\textdegree{}C
        above the critical threshold — trading some efficiency for increased
        operational stability margin.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.96\linewidth]{figures/thermo_trajectories.png}
  \caption{%
    Thermodynamic state trajectories for 100~evaluation episodes,
    grouped by EAF/BOF exhaust temperature regime.
    Mean $\pm1\sigma$ bands shown.
    The Lagrangian constraint floor ($T_{\text{comp,in}} > 33$\textdegree{}C)
    is marked by the dashed red line in the compressor inlet panel.%
  }
  \label{fig:thermo_trajectories}
\end{figure}

%%--------------------------------------------------------------------------
\subsection{FNO Surrogate: Fidelity Analysis and Remediation}
\label{subsec:fno_fidelity}

The FNO surrogate path is a core project objective: NVIDIA PhysicsNeMo's
FNO implementation enables GPU-vectorised training at ${\approx}10^6$~steps/s,
versus ${\approx}800$~steps/s on the CPU FMU path — a $1{,}250\times$
throughput gain that would enable dramatically richer hyperparameter search
and curriculum exploration.

\textbf{Version~1: Failure due to data degeneracy.}
The first FNO training run used a 75{,}000-trajectory dataset collected with
an incorrect initial-condition handling: the \texttt{reset()} method silently
ignored the LHS samples passed via the \texttt{options} dictionary, causing
all trajectories to start from the same default operating point.
Inspection revealed only 2{,}100 unique initial condition rows among
75{,}000 dataset entries — the dataset was effectively 35$\times$-replicated
from a tiny seed collection.
The FNO overfit to the repeated sequence structure, producing:

\begin{table}[h]
\centering
\caption{FNO surrogate fidelity metrics — Version~1 (degenerate 75K dataset).
         Fidelity gate thresholds: normalized RMSE $\leq 0.10$, $R^2 \geq 0.80$
         overall; $R^2 \geq 0.95$ for critical variables.}
\label{tab:fidelity_v1}
\begin{tabular}{lccc}
\toprule
Variable & Norm.\ RMSE & $R^2$ & Passed \\
\midrule
$T_{\text{comp,in}}$   & $0.276$ & $-0.549$ & No \\
$T_{\text{turb,in}}$   & $0.351$ & $-0.487$ & No \\
$T_{\text{comp,out}}$  & $0.257$ & $+0.081$ & No \\
$T_{\text{turb,out}}$  & $0.385$ & $-0.169$ & No \\
$W_{\text{turbine}}$   & $0.162$ & $-0.442$ & No \\
$W_{\text{comp}}$      & $0.132$ & $-0.047$ & No \\
$\eta_{\text{comp}}$   & $<0.001$ & $-85.6$ & No \\
$\eta_{\text{recup}}$  & $<0.001$ & $-992.7$ & No \\
$Q_{\text{recup}}$     & $0.214$ & $-0.557$ & No \\
$P_{\text{high}}$      & $0.000$ & $+1.000$ & Yes \\
\midrule
\textbf{Overall} & $\mathbf{0.197}$ & $\mathbf{-77.15}$ & \textbf{No} \\
\bottomrule
\end{tabular}
\end{table}

Negative $R^2$ values indicate the surrogate performs \emph{worse} than a
constant-mean predictor on held-out FMU trajectories.
Note the curious $P_{\text{high}} R^2 = +1.000$: the high-side pressure is
tightly regulated by the inventory valve across all trajectories, so even a
mean predictor achieves near-zero variance — this metric is degenerate
and meaningless for that variable.

\textbf{Root cause analysis.}
The \texttt{SCO2FMUEnv.reset()} method accepted an \texttt{options}
dictionary from the \texttt{TrajectoryCollector} containing LHS samples,
but applied them only to the FMU parameter table — not to the observation
normalisation initial conditions.
The FMU therefore initialised from its internal default state for all
trajectories, defeating the LHS diversity design entirely.

\textbf{Version~2: Remediation with 76{,}600 unique LHS trajectories.}
After diagnosing and fixing the \texttt{reset()} LHS application logic,
a new collection run was initiated targeting 100{,}000 unique trajectories.
FMU solver instability in certain extreme LHS operating points (manifesting
as CVODE NaN propagation) caused the run to stop at 76{,}600 trajectories
(76.6\% of target), yielding 3.98~GB of genuinely diverse data.

The PhysicsNeMo FNO (546{,}190 parameters, spectral convolutions over
719~timesteps) was trained on this V2 dataset:
\begin{itemize}
  \item Dataset: $N = 76{,}600$ trajectories, $T = 720$~steps each;
        80/10/10 train/validation/test split.
  \item Hardware: NVIDIA DGX Spark GB10 Grace Blackwell GPU.
  \item Optimizer: Adam (lr~$= 10^{-3}$, weight decay~$= 10^{-4}$),
        200~epochs, early-stop patience~20.
  \item Per-variable z-score normalisation applied before training.
  \item Training loss (MSE) converged from 0.122 (epoch~1) to 0.000286
        (epoch~30), with validation loss 0.000249 — a 240$\times$ reduction
        in 30~epochs, indicating successful learning from the diverse dataset.
\end{itemize}

\textbf{Version~2 results: Fidelity gate passed.}
Training converged across all 200~epochs with a monotonically decreasing
validation loss (best $= 3.7 \times 10^{-5}$ at epoch~200).
The fidelity gate evaluated on the 7{,}660-trajectory held-out test split yielded:

\begin{table}[h]
\centering
\caption{FNO surrogate fidelity metrics — Version~2 (76{,}600 unique LHS trajectories).
         Fidelity gate thresholds: normalized RMSE $\leq 0.10$, $R^2 \geq 0.80$.
         Training: NVIDIA DGX Spark GB10, 200 epochs, 54 minutes.}
\label{tab:fidelity_v2}
\begin{tabular}{lcc}
\toprule
Metric & V1 (Degenerate) & V2 (Remediated) \\
\midrule
Overall norm.\ RMSE & 0.197 & \textbf{0.0010} \\
Overall $R^2$        & $-77.15$ & $\mathbf{1.0000}$ \\
Best val.\ loss (MSE)& $-$  & $3.7 \times 10^{-5}$ \\
FNO parameters       & 546{,}190 & 546{,}190 \\
Fidelity gate        & \textcolor{red}{FAILED} & \textcolor{green!60!black}{\textbf{PASSED}} \\
\bottomrule
\end{tabular}
\end{table}

The 200$\times$ improvement in normalised RMSE (from 0.197 to 0.001) and the
$R^2$ of 1.000 confirm that the PhysicsNeMo FNO successfully learned the
sCO\textsubscript{2} cycle dynamics from the V2 dataset.
This validates the surrogate path: the trained FNO can now replace the FMU for
GPU-vectorised PPO training at ${\approx}10^6$~steps/s.

The key lesson is that \emph{data quality entirely dominates FNO performance}.
Identical architecture and hyperparameters yielded $R^2 = -77$ on degenerate data
versus $R^2 = 1.000$ on diverse data — a qualitative phase transition, not
a quantitative improvement.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.82\linewidth]{figures/fidelity_rmse.png}
  \caption{%
    FNO surrogate fidelity: normalised RMSE per state variable (Version~1,
    degenerate 75K dataset).
    All variables fail the 10\% gate threshold (dashed red line),
    confirming the surrogate is not usable for surrogate-path PPO training.
    Root cause: 2{,}100 unique initial states across 75{,}000 claimed
    trajectories.
    Version~2 (76{,}600 unique LHS trajectories): overall $R^2 = 1.000$,
    RMSE $= 0.0010$ — fidelity gate passed.%
  }
  \label{fig:fidelity}
\end{figure}

%%--------------------------------------------------------------------------
\subsection{Deployment Latency}
\label{subsec:latency}

The final policy is exported via PyTorch $\to$ ONNX $\to$ TensorRT FP16.
Latency is measured over 1{,}000 iterations on the NVIDIA DGX Spark GB10:

\begin{table}[h]
\centering
\caption{TensorRT FP16 inference latency (1{,}000 measurement iterations,
         Phase~3 checkpoint, NVIDIA DGX Spark GB10 Grace Blackwell).
         Input: 100-dimensional observation vector (20~variables $\times$
         5~history steps).
         Output: 5-dimensional normalised action.}
\label{tab:latency}
\begin{tabular}{lc}
\toprule
Latency percentile & Value \\
\midrule
p50 & $0.038$~ms \\
p90 & $0.043$~ms \\
p99 & $\mathbf{0.046}$~ms \\
Throughput & ${\approx}29{,}600$~queries/s \\
\bottomrule
\end{tabular}
\end{table}

The p99 latency of 0.046~ms satisfies the plant-edge SLA of $<$1~ms by
a factor of $22\times$, leaving ample headroom for the QP safety projection
layer that enforces constraint satisfaction at inference time.
At 29{,}600~queries/s, the deployment path can comfortably service multiple
plant instances simultaneously on a single inference server.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/latency_summary.png}
  \caption{TensorRT FP16 inference latency percentiles.
           p99 of 0.046~ms is $22\times$ under the 1~ms plant-edge SLA.}
  \label{fig:latency}
\end{figure}

%% ---------------------------------------------------------------------------
